Edge-Mobile Anomaly Detection Apple - Project Context
====================================================

1. PROJECT OVERVIEW
==================

Project: Mobile Network Anomaly Detection Pipeline Optimized for Apple Silicon
Purpose: Academic research for CMMSE 2025 conference submission
Repository: https://github.com/josemifv/Edge-Mobile-Anomaly-Detection-Apple.git
Branch: cmmse2025 (current development branch)
Python Version: 3.13.2 (managed with uv tool)

Academic Focus:
- Enhanced mobile network anomaly detection pipeline
- Hardware optimization for Apple Silicon (M-series processors) 
- OSP (Orthogonal Subspace Projection) based anomaly detection
- Research-grade implementation with reproducible results

2. CURRENT REPOSITORY STRUCTURE
===============================

Edge-Mobile-Anomaly-Detection-Apple/
├── .git/                          # Git version control
├── .gitignore                     # Git ignore patterns
├── .python-version                # Python 3.13 specification
├── .venv/                         # Virtual environment (uv-managed)
├── LICENSE                        # MIT License (José Miguel Franco-Valiente)
├── README.md                      # Complete project documentation
├── requirements.txt               # Python dependencies
├── backup/                        # Backup directory (git-ignored)
├── data/                          # Data storage
│   ├── raw@ -> ../../datasets/milan_telecom/raw  # Symlink to raw data
│   └── processed/                 # Processed data files
│       ├── ingested_data.parquet     # Stage 1 output (4.4GB, 319.9M rows)
│       ├── preprocessed_data.parquet # Stage 2 output (2.0GB, 89.2M rows)
│       └── reference_weeks.parquet   # Stage 3 output (479KB, 39.4K refs)
├── results/                       # Analysis outputs and results
│   ├── anomalies.parquet             # Standard anomaly detection results
│   ├── full_individual_anomalies.parquet  # Individual anomaly records (148MB)
│   ├── severe_anomalies_top_*.csv    # Severity analysis reports
│   ├── figures/                      # Visualization outputs
│   ├── benchmarks/                   # Performance benchmark results
│   └── data/                         # Micro test results by configuration
└── scripts/                       # Core pipeline + utility scripts
    ├── 01_data_ingestion.py             # Stage 1: Data loading & preprocessing
    ├── 02_data_preprocessing.py         # Stage 2: Aggregation & validation
    ├── 03_week_selection.py             # Stage 3: Reference week selection
    ├── 04_anomaly_detection_individual.py   # Stage 4: Individual anomaly detection
    ├── 05_analyze_anomalies.py          # Stage 5: Comprehensive anomaly analysis
    ├── 06_generate_anomaly_map.py       # Stage 6: Geographic anomaly visualization
    ├── 07_plot_extreme_cell_timeline.py # Stage 7: Extreme cell timeline analysis
    ├── analyze_severe_anomalies.py      # Severity analysis tools
    ├── benchmark_parameter_sweep.py     # Comprehensive parameter optimization
    ├── benchmark_micro_test.py          # Quick system validation
    ├── utils_cell_aggregation.py        # Utility: Cell-level data aggregation
    ├── demo_cell_aggregation.py         # Demo: Performance comparison Polars vs Pandas
    └── run_pipeline.py                  # Complete 6-stage pipeline orchestrator

3. CORE PIPELINE ARCHITECTURE (5 STAGES - REFACTORED)
====================================================

Stage 1: Data Ingestion (01_data_ingestion.py)
- Input: Raw .txt files from Milano Telecom dataset
- Processing: Parallel file loading optimized for Apple Silicon
- Features: Timestamp conversion, column standardization, data type optimization
- Output: ingested_data.parquet (319.9M rows, 4.4GB)
- Performance: 1.77M rows/second (181.02s execution)

Stage 2: Data Preprocessing (02_data_preprocessing.py)
- Input: Ingested data from Stage 1
- Processing: Cell-wise aggregation, directional column merging, validation
- Features: SMS/call consolidation, quality checks, 72% data compression
- Output: preprocessed_data.parquet (89.2M rows, 2.0GB)
- Performance: 564K rows/second (159.17s execution)

Stage 3: Reference Week Selection (03_week_selection.py)
- Input: Preprocessed data from Stage 2
- Processing: MAD (Median Absolute Deviation) analysis for normal week identification
- Features: ISO week numbering, statistical outlier detection, configurable thresholds
- Output: reference_weeks.parquet (39.4K reference weeks)
- Performance: 795K rows/second (112.27s execution)

Stage 4: Individual Anomaly Detection (04_anomaly_detection_individual.py) [POLARS OPTIMIZED]
- Input: Preprocessed data + reference weeks from Stages 2 & 3
- Processing: OSP with individual anomaly record capture using Polars + NumPy
- Features: Vectorized operations, parallel processing, float32 optimization
- Output: individual_anomalies.parquet (individual anomaly records with full details)
- Performance: 783K samples/second (114s for 89.2M samples, 5.6M anomalies detected)

Stage 5: Comprehensive Anomaly Analysis (05_analyze_anomalies.py) [NEW]
- Input: Individual anomaly records from Stage 4
- Processing: Statistical analysis, pattern identification, visualization generation
- Features: Temporal analysis, severity ranking, cell-level patterns, report generation
- Output: Analysis reports, visualizations, and research-grade insights
- Performance: Fast analysis of detected anomalies

4. ALGORITHM IMPLEMENTATION
==========================

OSP (Orthogonal Subspace Projection) Anomaly Detection:
- Mathematical foundation: SVD decomposition X = UΣV^T
- Normal subspace projection with reconstruction error calculation
- Anomaly scoring: ||residuals||_2 compared to threshold
- Per-cell training using reference weeks from Stage 3
- Configurable parameters:
  * n_components: SVD dimensions (default: 3)
  * anomaly_threshold: Standard deviation multiplier (default: 2.0)
  * standardize: Feature standardization (default: True)

Enhanced Individual Tracking (04_anomaly_detection_osp_detailed.py):
- Individual anomaly record capture with timestamps
- Severity scoring: (anomaly_score - training_mean) / training_std
- Traffic feature preservation for analysis
- Excess factor calculation (threshold multiples)

5. PERFORMANCE ACHIEVEMENTS (Apple Silicon M4 Pro)
==================================================

Dataset: Milano Telecom (62 files, 319.9M rows, Nov 2013 - Jan 2014)

Pipeline Performance:
- Stage 1: 181.02s | 1.77M rows/sec | 319.9M → 319.9M rows
- Stage 2: 159.17s | 564K rows/sec | 319.9M → 89.2M rows (72% compression)
- Stage 3: 112.27s | 795K rows/sec | 39.4K reference weeks selected
- Stage 4: 469.97s | 190K samples/sec | 5.65M anomalies (6.33% rate)
- Total: 922.43s (15.37 minutes) | 347K rows/sec overall

Key Metrics:
- Success rate: 100% (10,000/10,000 cells processed)
- Data compression: 72% through aggregation
- Anomaly detection rate: 6.33% average across all cells
- Memory efficiency: 5.6GB peak usage
- Reference weeks: 39.4K selected (≈4 per cell average)

Anomaly Severity Analysis:
- Severity range: 2.0σ to 5,717σ (standard deviations above normal)
- Most severe cell: 5240 (multiple extreme anomalies)
- Peak anomaly patterns: Night hours (01:00-02:00), weekdays
- Traffic characteristics: High call volumes without proportional SMS/internet

6. APPLE SILICON OPTIMIZATIONS
==============================

Hardware Acceleration:
- Native ARM64 compilation (aarch64 architecture)
- PyTorch MPS (Metal Performance Shaders) support enabled
- Automatic CPU core detection and utilization
- Optimized multiprocessing for M-series processors

Software Optimizations:
- NumPy/SciPy ARM64 optimized versions
- scikit-learn Apple Silicon compilation
- Parallel processing strategies for file I/O and computation
- Memory-efficient data structures and operations

Configuration:
- Default workers: Auto-detected (8-14 for M-series)
- Virtual environment: Python 3.13.2 with uv package manager
- Dependencies: All Apple Silicon native packages

7. DEVELOPMENT TOOLS & ENVIRONMENT
==================================

Python Environment:
- Python 3.13.2 (specified in .python-version)
- Package manager: uv 0.6.14 (Astral's fast tool)
- Virtual environment: .venv/ (properly isolated)
- Dependencies: requirements.txt (traditional approach for academic simplicity)

Core Dependencies:
- pandas ≥2.3.0 (data processing)
- numpy ≥2.2.6 (numerical computing)
- pyarrow ≥20.0.0 (parquet I/O)
- scikit-learn ≥1.7.0 (machine learning)
- torch ≥2.2.0 (MPS support for Apple Silicon)
- matplotlib ≥3.10.3, seaborn ≥0.13.2 (visualization)
- psutil ≥7.0.0 (system monitoring)

Development Tools:
- pytest ≥8.0.0 (testing framework)
- black ≥24.1.0 (code formatting)
- isort ≥5.13.0 (import sorting)

8. ACADEMIC RESEARCH FEATURES
=============================

CMMSE 2025 Conference Focus:
- Clean, documented code structure for academic publication
- Reproducible results with parameter control
- Comprehensive performance benchmarking
- Individual anomaly analysis capabilities

Research Tools:
- Parameter sweep framework (benchmark_parameter_sweep.py)
- Quick validation testing (benchmark_micro_test.py)
- Severity analysis with visualizations (analyze_severe_anomalies.py)
- Individual anomaly tracking for case studies

Documentation:
- Complete README with usage examples
- Inline code documentation and academic comments
- Performance results and configuration tables
- Research-grade implementation standards

9. CURRENT PROJECT STATUS
=========================

Development Branch: cmmse2025
Last Commit: 65f792c - "feat: Optimize Stage 4 with Polars migration and performance improvements"

Refactored Components:
✅ REFACTORED: 5-stage pipeline architecture (Stage 4 + new Stage 5)
✅ NEW: Individual anomaly detection without aggregation (Stage 4)
✅ NEW: Comprehensive anomaly analysis framework (Stage 5)
✅ NEW: Geographic anomaly visualization with Milano grid (Stage 6)
✅ NEW: Extreme cell timeline analysis (Stage 7)
✅ SIMPLIFIED: Single pipeline runner (run_pipeline.py) for 5-stage execution
✅ Apple Silicon optimization verified and documented
✅ Complete dataset processing (100% success rate)
✅ Enhanced individual anomaly analysis capabilities
✅ Interactive maps with percentile-based classification
✅ Comprehensive timeline visualizations for extreme cells
✅ Comprehensive benchmarking tools
✅ Academic documentation for CMMSE 2025
✅ Clean repository structure with minimal dependencies
✅ CODE REVIEW FIXES: All improvements from codex_review.md implemented
  • Fixed warnings filtering to be more specific and scoped
  • Updated README.md to reflect 5-stage pipeline architecture
  • Fixed ranking logic in 03_week_selection.py for clarity
  • Optimized CSV reading performance in 01_data_ingestion.py
  • Fixed hardcoded cell ID limits in 06_generate_anomaly_map.py
  • Standardized timing functions to use time.perf_counter()
  • Fixed Pool arguments to use explicit processes= parameter
  • Requirements.txt already had missing dependencies added

Refactoring Benefits:
✅ Separation of detection and analysis concerns
✅ Individual anomaly records for detailed research
✅ Flexible analysis capabilities for different research questions
✅ Better suited for academic publication and case studies

Testing Status:
✅ End-to-end pipeline verification completed
✅ Performance benchmarking on full dataset
✅ Individual anomaly analysis validated
✅ Micro testing framework operational
✅ NEW: 5-stage refactored pipeline successfully tested
✅ COMPLETE: Full dataset 5-stage pipeline execution (10,000 cells)
✅ CLEAN: Repository cleaned of all generated files (ready for production)

Full Pipeline Test Results (Complete Dataset - 10,000 cells):
- Stage 1: 81.49s | 3.96M rows/sec | 319.9M rows processed
- Stage 2: 107.95s | 829K rows/sec | 89.2M rows output (72% compression)
- Stage 3: 7.33s | Reference week selection for 10,000 cells (39.5K weeks)
- Stage 4: ~6-8 minutes | 10,000 cells processed | 4.86M individual anomalies detected
- Stage 5: 10.58s | Complete analysis and visualization generation
- Total: ~15-17 minutes for complete dataset
- Anomaly rate: 5.45% | Severity range: 2.0σ to 7,791σ | Peak times: 16:00-17:00

Validated Analysis Results:
- Total anomalies detected: 4,864,815
- Unique cells with anomalies: 10,000 (100% coverage)
- Date range: 2013-10-31 to 2014-01-01
- Peak anomaly times: Rush hours (16:00-17:00) and morning hours (07:00-11:00)
- Weekday bias: Tuesday/Wednesday peak, weekend reduced activity
- Top anomalous cells: 4472 (4,524 anomalies), 4772 (4,521 anomalies)
- Generated outputs: Summary report, severity distribution plot, hourly pattern visualization

Repository Health:
✅ Clean commit history with academic focus
✅ Proper dependency management with uv
✅ Data directories excluded from version control
✅ Backup system implemented
✅ Documentation complete and current

10. USAGE EXAMPLES
==================

Individual Stage Execution:
```bash
# Stage 1: Data Ingestion
uv run scripts/01_data_ingestion.py data/raw/ --output_path data/processed/ingested_data.parquet

# Stage 2: Data Preprocessing  
uv run scripts/02_data_preprocessing.py data/processed/ingested_data.parquet --output_path data/processed/preprocessed_data.parquet

# Stage 3: Reference Week Selection
uv run scripts/03_week_selection.py data/processed/preprocessed_data.parquet --output_path data/processed/reference_weeks.parquet

# Stage 4: Individual Anomaly Detection
uv run scripts/04_anomaly_detection_individual.py data/processed/preprocessed_data.parquet data/processed/reference_weeks.parquet --output_path results/individual_anomalies.parquet
```

Complete Pipeline:
```bash
# Run complete 5-stage pipeline
uv run scripts/run_pipeline.py data/raw/ --output_dir results/

# With custom parameters
uv run scripts/run_pipeline.py data/raw/ --output_dir results/ --n_components 5 --anomaly_threshold 2.5 --preview
```

Enhanced Analysis:
```bash
# Generate comprehensive analysis
uv run scripts/05_analyze_anomalies.py results/individual_anomalies.parquet --output_dir results/

# Analyze severe anomalies
uv run scripts/analyze_severe_anomalies.py results/individual_anomalies.parquet --top_n 20 --generate_plots --export_report
```

11. NEXT RESEARCH DIRECTIONS
============================

Potential Extensions:
- Additional anomaly detection algorithms (Isolation Forest, Autoencoders)
- Time series feature engineering enhancements
- Spatial correlation analysis between cells
- Real-time anomaly detection capabilities
- Comparative studies with other hardware architectures

Academic Applications:
- Network incident investigation frameworks
- Capacity planning optimization
- Security monitoring for telecom networks
- Quality of service enhancement strategies

Publication Ready:
- Code structure optimized for academic review
- Comprehensive documentation and examples
- Reproducible results with clear methodology
- Performance benchmarks for Apple Silicon architecture

12. STAGE 4 OUTPUT ANALYSIS COMPLETED
=====================================

Task: Analyze Stage 4 output format and structure for Stage 6 aggregation
Status: ✅ COMPLETED

Key Findings:
- Output File: results/04_individual_anomalies.parquet (147 MB, 4.84M records)
- Format: Polars-optimized parquet with columnar storage
- Columns: cell_id, timestamp, anomaly_score, sms_total, calls_total, internet_traffic, severity_score
- Coverage: 100% (10,000 cells), 5.43% detection rate
- Data Quality: Complete (no missing values), ready for aggregation
- Performance: 0.066s load time, optimized for Stage 6 processing

Critical Stage 6 Considerations:
✅ Cell ID format needs flattening (currently List(Int64))
✅ Severity range: 2σ to 16,301σ (excellent discrimination)
✅ Timestamp precision: millisecond (datetime64[ms])
✅ Traffic metrics: Full feature set preserved
✅ Aggregation ready: Multiple strategies supported

Documentation: stage4_output_analysis.md created with comprehensive analysis

13. STEP 2: CELL-LEVEL AGGREGATION LOGIC COMPLETED
===================================================

Task: Design cell-level aggregation logic to transform individual anomaly records into cell statistics
Status: ✅ COMPLETED

Implementation Details:
- Created utils_cell_aggregation.py with both Polars and Pandas implementations
- Efficient groupby operations for 4.84M individual anomaly records
- Transforms data from List(Int64) cell_id format to flat aggregated statistics

Required Metrics Implemented:
✅ anomaly_count: count() of records per cell
✅ avg_severity: mean(severity_score) 
✅ max_severity: max(severity_score)
✅ severity_std: stddev(severity_score)
✅ avg_sms_total: mean(sms_total)
✅ avg_calls_total: mean(calls_total)
✅ avg_internet_traffic: mean(internet_traffic)

Performance Results:
- Polars Implementation: 70.7M records/second (0.068s)
- Pandas Implementation: 1.98M records/second (2.445s)
- Polars is 35.7x faster than Pandas for this aggregation
- Output: 10,000 cells with complete statistics

Files Created:
✅ scripts/utils_cell_aggregation.py - Main aggregation script with both implementations
✅ scripts/demo_cell_aggregation.py - Performance comparison demonstration
✅ results/06_cell_statistics.parquet - Polars aggregation output
✅ results/06_cell_statistics_pandas.parquet - Pandas aggregation output

Key Features:
- Handles List(Int64) cell_id flattening automatically
- Null value handling for severity_std (single anomaly cases)
- Comprehensive error handling and progress reporting
- Academic-grade documentation and performance metrics
- Both Polars and Pandas implementations for comparison

Validation Results:
- Successfully processed 4,841,471 individual anomaly records
- Generated statistics for all 10,000 unique cells
- Average 484.1 anomalies per cell
- Severity range: 2.25σ to 16,301σ
- Data integrity verified between Polars and Pandas results

14. STEP 3: UPDATE IMPORTS AND DEPENDENCIES COMPLETED
=====================================================

Task: Update imports and dependencies for enhanced parquet reading capabilities
Status: ✅ COMPLETED

Implementation Details:
- Created imports_configuration.py with comprehensive parquet support setup
- Verified existing dependencies in requirements.txt are sufficient:
  • pandas>=2.3.0 ✅
  • pyarrow>=20.0.0 ✅ 
  • polars>=0.20.0 ✅

Optimized Import Configuration:
```python
import pandas as pd
import pyarrow.parquet as pq  # For parquet support
import polars as pl  # Optional: for faster processing
```

Parquet Capabilities Verified:
✅ pandas version: 2.3.0 with pyarrow engine
✅ pyarrow version: 20.0.0 with full parquet support
✅ polars version: 1.31.0 with pyarrow backend
✅ Multi-threading and memory mapping enabled
✅ Optimized compression (zstd) configured

Configuration Features:
- Enhanced pandas parquet reading with nullable dtypes
- PyArrow direct access for high-performance operations
- Polars lazy evaluation for large datasets
- Memory-efficient chunked reading for massive files
- Optimized writing functions with compression

Files Created:
✅ imports_configuration.py - Complete parquet configuration and utilities
✅ Verification functions for testing parquet support
✅ Example usage patterns for all three libraries
✅ Performance optimization settings

Project Parquet Usage:
- Stage 1: Polars with pyarrow backend for data ingestion
- Stage 2-3: Polars scan_parquet for lazy evaluation
- Stage 4: Mixed pandas/polars for anomaly detection
- Stage 5-6: Pandas for analysis and visualization
- All stages use zstd compression for optimal storage

Performance Impact:
- Existing pipeline already optimized for Apple Silicon
- Parquet I/O is 35.7x faster with Polars vs Pandas
- Memory mapping and multi-threading enabled
- Ready for production-scale data processing

15. STEP 4: MODIFY load_anomaly_data METHOD COMPLETED
=====================================================

Task: Modify load_anomaly_data method to handle parquet input and aggregate individual anomaly records
Status: ✅ COMPLETED

Implementation Details:
- Modified load_anomaly_data method in scripts/06_generate_anomaly_map.py
- Changed input parameter from anomaly_csv_path to anomaly_parquet_path
- Updated file reading from pd.read_csv() to pd.read_parquet()
- Added proper aggregation logic grouping by cell_id with statistics:
  • severity_score: count, mean, max, std
  • sms_total, calls_total, internet_traffic: mean
- Renamed columns to match expected format:
  • cell_id, anomaly_count, avg_severity, max_severity, severity_std
  • sms_total_mean, calls_total_mean, internet_traffic_mean
  
Key Features:
✅ Handles individual anomaly records from Stage 4 output (parquet format)
✅ Transforms individual records into cell-level statistics for visualization
✅ Maintains compatibility with existing map generation pipeline
✅ Provides clear progress reporting during data loading and aggregation
✅ Supports the refactored 5-stage pipeline architecture

Testing Ready:
- Method ready for integration with Stage 4 individual anomaly parquet output
- Compatible with existing geographic visualization workflow
- Maintains expected data format for downstream map generation

15. STEP 6: DATA VALIDATION SYSTEM COMPLETED
==================================================

Task: Add comprehensive data validation checks for parquet files, data types, ranges, missing values, and Cell ID consistency with GeoJSON
Status: ✅ COMPLETED

Implementation Details:
- Created comprehensive data validation module (scripts/data_validation.py)
- Implemented pipeline integration script (scripts/validate_pipeline_data.py)
- Added support for all pipeline stage schemas (ingested, preprocessed, reference_weeks, individual_anomalies, cell_statistics)
- Created comprehensive documentation (VALIDATION_GUIDE.md)

Validation Features Implemented:
✅ Required columns validation for all pipeline stages
✅ Data types and ranges validation with flexible type compatibility
✅ Missing values handling with percentage-based thresholds
✅ Cell ID consistency with GeoJSON support (handles both direct integer and array formats)
✅ Timestamp validation and duplicate detection
✅ Stage-specific validations (anomaly data, aggregated data)
✅ Cross-stage consistency checks
✅ Comprehensive reporting with errors, warnings, and summary statistics

Key Validation Capabilities:
- Schema auto-detection for pipeline files
- Handles complex data types (arrays, lists, objects)
- Performance-optimized for large datasets (multi-GB files)
- Batch validation of multiple files
- Detailed validation reports with actionable insights
- Integration-ready for CI/CD pipelines

Validation Coverage:
- File existence and format validation
- Schema compliance (required columns, data types)
- Data quality (null values, ranges, duplicates)
- Geographic consistency (cell ID vs GeoJSON)
- Temporal consistency (timestamp ranges)
- Cross-stage data consistency
- Performance metrics and memory usage

Files Created:
✅ scripts/data_validation.py - Core validation module (570 lines)
✅ scripts/validate_pipeline_data.py - Pipeline integration script (219 lines)
✅ VALIDATION_GUIDE.md - Comprehensive documentation (364 lines)

Testing Results:
✅ Successfully validated individual anomaly file (4.84M records)
✅ Successfully validated multiple pipeline files (5 stages)
✅ Detected real data quality issues (null values in raw data)
✅ Generated comprehensive summary reports
✅ Confirmed cross-stage data consistency

Validation Performance:
- Individual anomalies file (147MB): 0.52s validation time
- Preprocessed data file (1.9GB): 2.04s validation time
- Complete pipeline validation: ~3-4 seconds total
- Memory efficient: handles large datasets without issues

Integration Ready:
- Can be used standalone or integrated into pipeline stages
- Supports both automated and manual validation workflows
- Provides exit codes for CI/CD integration
- Generates machine-readable validation results

17. STEP 7: VISUALIZATION FUNCTIONALITY TESTING COMPLETED
==========================================================

Task: Test visualization functionality with sample Stage 4 output
Status: ✅ COMPLETED

Implementation Details:
- Created comprehensive test script (test_visualization_functionality.py) with 10 test categories
- Fixed cell_id array format handling in map generation script
- Generated complete interactive map suite with both Folium and Plotly visualizations
- Validated all classification distributions, color schemes, popup information, and legends

Test Results Summary:
✅ Data Loading: 4,841,471 individual anomaly records loaded successfully
✅ Data Aggregation: 0.239s processing time, 20.3M records/second
✅ Classification: Percentile-based classification with 5-6 meaningful categories
✅ Color Schemes: Valid hex format with green→red progression
✅ Folium Maps: Interactive HTML maps with full popups and tooltips
✅ Plotly Maps: Choropleth/scatter maps with continuous color scales
✅ Popup Information: Rich 8-field popups with formatted cell data
✅ Legends: Color-coded with category counts
✅ Metrics Validation: Statistical integrity maintained, no negative values
✅ Performance: 18.31s total processing time for complete visualization pipeline

Generated Files (8 total):
✅ Milano anomaly count Folium map (25.1 MB)
✅ Milano severity Folium map (24.9 MB)
✅ Milano anomaly count Plotly map (6.1 MB)
✅ Milano severity Plotly map (5.2 MB)
✅ Cell classification datasets (2 CSV files)
✅ Statistical summary reports (2 TXT files)

Key Validation Metrics:
- Total cells processed: 10,000 (100% coverage)
- Classification categories: 5-6 per metric with balanced distribution
- Color progression: Intuitive green→yellow→orange→red scheme
- Interactive features: All popups, tooltips, and legends functional
- Statistical integrity: All data quality checks passed

Production Readiness:
✅ Robust Stage 4 output format handling
✅ Full Milano grid geographic coverage
✅ Responsive interactive features
✅ Performance suitable for production workloads
✅ Comprehensive validation and error handling

Files Created:
✅ test_visualization_functionality.py - Comprehensive test suite
✅ visualization_test_final_report.md - Complete validation report
✅ 8 visualization output files in results/test_maps/
✅ Fixed scripts/06_generate_anomaly_map.py - Cell ID array handling

Academic Impact:
- CMMSE 2025 conference ready for submission
- Production-ready visualization system for academic publication
- Real-time anomaly monitoring system applicable
- Complete test coverage for peer review

==========================================
Status: ✅ COMPLETE - CMMSE 2025 Ready
Last Updated: June 2024 - Visualization Testing Completed
==========================================

18. STEP 8: DOCUMENTATION AND EXAMPLES UPDATE COMPLETED
========================================================

Task: Update documentation and examples with new data flow and geographic visualization
Status: ✅ COMPLETED

Implementation Details:
- Updated docstrings and inline comments in 06_generate_anomaly_map.py
- Enhanced script header with comprehensive usage examples and data flow explanation
- Added detailed inline comments explaining cell-level aggregation process
- Updated README.md with 6-stage pipeline architecture
- Added comprehensive Geographic Visualization section with features and usage
- Updated .context file with Step 8 completion status

Documentation Updates:
✅ Enhanced script docstring with data flow explanation (Input → Processing → Output)
✅ Added comprehensive usage examples including pipeline integration
✅ Detailed inline comments for load_anomaly_data method explaining 4-step process
✅ Updated README.md pipeline stages section (5 → 6 stages)
✅ Added Geographic Visualization Features section with technical details
✅ Included map types, classification metrics, and example outputs
✅ Added usage examples for geographic visualization workflows

Key Features Documented:
- Stage 6 data flow: Individual anomalies → Cell aggregation → Classification → Mapping
- Interactive map generation with Folium and Plotly
- Percentile-based classification system
- Milano grid integration with full geographic coverage
- Cell-level statistical aggregation process

Example Commands Added:
```bash
# Basic usage with Stage 4 output
python scripts/06_generate_anomaly_map.py results/individual_anomalies.parquet --output_dir results/maps/

# Custom metrics and classification  
python scripts/06_generate_anomaly_map.py results/individual_anomalies.parquet \
    --output_dir maps/ --metrics anomaly_count max_severity

# Complete pipeline with geographic visualization
python scripts/run_pipeline.py data/raw/ --output_dir results/
python scripts/06_generate_anomaly_map.py results/individual_anomalies.parquet --output_dir results/maps/
```

Readability Improvements:
- Clear separation of data flow steps with numbered explanations
- Comprehensive argument descriptions with defaults and examples
- Integration examples showing pipeline connectivity
- Technical details balanced with user-friendly explanations
- Academic-grade documentation suitable for CMMSE 2025 submission

Files Updated:
✅ scripts/06_generate_anomaly_map.py - Enhanced docstrings and inline comments
✅ README.md - Added 6-stage pipeline documentation and geographic visualization section  
✅ .context - Updated with Step 8 completion status

Ready for Production:
- Complete documentation for academic publication
- Clear usage examples for all integration scenarios
- Comprehensive technical documentation for peer review
- User-friendly examples for practical implementation

==========================================
Status: ✅ COMPLETE - CMMSE 2025 Ready
Last Updated: December 2024 - Documentation Update Completed
==========================================

19. SCRIPT ORGANIZATION CLEANUP COMPLETED
=========================================

Task: Reorganize utility scripts to avoid confusion with pipeline stage numbering
Status: ✅ COMPLETED

Changes Made:
- Renamed 06_cell_aggregation.py → utils_cell_aggregation.py
- Renamed 06_cell_aggregation_demo.py → demo_cell_aggregation.py
- Updated all internal filename references in the renamed scripts
- Updated .context file to reflect new script organization

Rationale:
- Pipeline stages should use numbered prefixes (01_, 02_, 03_, etc.)
- Utility scripts should use descriptive prefixes (utils_, demo_, etc.)
- This prevents confusion about what is part of the main pipeline vs utility tools

Current Script Organization:
✅ Pipeline Scripts: 01_ through 07_ (numbered stages)
✅ Utility Scripts: utils_ prefix (standalone tools)
✅ Demo Scripts: demo_ prefix (performance comparisons)
✅ Analysis Scripts: analyze_ prefix (research tools)
✅ Benchmark Scripts: benchmark_ prefix (performance testing)

Clear Separation:
- 06_generate_anomaly_map.py = Stage 6 of the main pipeline
- utils_cell_aggregation.py = Utility script for cell aggregation
- demo_cell_aggregation.py = Performance comparison demo

Files Affected:
✅ scripts/utils_cell_aggregation.py (renamed from 06_cell_aggregation.py)
✅ scripts/demo_cell_aggregation.py (renamed from 06_cell_aggregation_demo.py)
✅ .context file updated with new organization

==========================================
Status: ✅ COMPLETE - Clean Script Organization
Last Updated: June 2025 - Script Naming Cleanup
==========================================

20. REPOSITORY REORGANIZATION COMPLETED
=======================================

Task: Clean up repository structure with organized utility scripts and reports
Status: ✅ COMPLETED

Changes Made:
✅ Moved utility scripts to scripts/utils/ directory:
   - scripts/utils/data_validation.py (data validation module)
   - scripts/utils/analyze_severe_anomalies.py (severity analysis tools)
   - scripts/utils/benchmark_parameter_sweep.py (parameter optimization)
   - scripts/utils/benchmark_micro_test.py (quick system validation)
   - scripts/utils/utils_cell_aggregation.py (cell-level aggregation)
   - scripts/utils/demo_cell_aggregation.py (performance comparison demo)
   - scripts/utils/validate_pipeline_data.py (pipeline validation)

✅ Created outputs/reports/ directory structure for organized report storage

✅ Cleaned output files from repository root

Benefits:
- Clear separation between pipeline stages and utility tools
- Better organization for academic publication
- Cleaner repository structure for CMMSE 2025 submission
- Centralized location for analysis reports and outputs

Updated Repository Structure:
Edge-Mobile-Anomaly-Detection-Apple/
├── scripts/
│   ├── 01_data_ingestion.py to 07_plot_extreme_cell_timeline.py (pipeline stages)
│   ├── run_pipeline.py (pipeline orchestrator)
│   └── utils/ (utility and analysis scripts)
│       ├── analyze_severe_anomalies.py
│       ├── benchmark_micro_test.py
│       ├── benchmark_parameter_sweep.py
│       ├── data_validation.py
│       ├── demo_cell_aggregation.py
│       ├── utils_cell_aggregation.py
│       └── validate_pipeline_data.py
└── outputs/
    └── reports/ (organized report storage)

==========================================
Status: ✅ COMPLETE - Repository Reorganized
Last Updated: June 2024 - Repository Cleanup
==========================================

21. STEP 1: BENCHMARK SCRIPT SCOPE AND INTERFACE COMPLETED
===========================================================

Task: Define Benchmark Script Scope and Interface - Create pipeline_benchmark_runner.py
Status: ✅ COMPLETED

Implementation Details:
- Created scripts/utils/pipeline_benchmark_runner.py with comprehensive CLI interface
- All required CLI arguments implemented: --runs (default 10), --output_dir, --max_workers, --n_components, --anomaly_threshold, --keep_tmp, --verbose
- Environment configuration support with .env file reading
- Re-uses existing run_pipeline.py with fixed parameters in a loop structure
- Organized output hierarchy implemented as specified:
  * outputs/benchmarks/YYYYMMDD_HHMMSS/run_{i}/ for raw logs & artifacts
  * outputs/benchmarks/YYYYMMDD_HHMMSS/summary/ for aggregated reports & plots

Key Features Implemented:
✅ CLI argument parsing with comprehensive help and examples
✅ Environment defaults loading from .env file when present
✅ Multi-run benchmark execution with error handling
✅ Timestamped directory hierarchy for organized output
✅ System information collection (Apple Silicon detection, CPU/memory info)
✅ Individual run metrics collection (timing, memory usage, output files)
✅ Comprehensive logging system with file and console outputs
✅ Summary report generation (JSON, CSV, Markdown, PNG plots)
✅ Temporary file cleanup options
✅ Performance visualization with matplotlib
✅ Detailed progress tracking and error reporting

Output Structure:
outputs/benchmarks/YYYYMMDD_HHMMSS/
├── run_1/, run_2/, ..., run_N/    # Individual run outputs
│   ├── logs/pipeline_execution.log    # Pipeline execution logs
│   ├── data/*.parquet                 # Pipeline stage outputs
│   ├── reports/                       # Analysis reports
│   └── run_metrics.json               # Run-specific metrics
├── summary/                           # Aggregated benchmark results
│   ├── benchmark_summary.json         # Complete benchmark statistics
│   ├── timing_analysis.csv            # Timing data for analysis
│   ├── performance_plot.png           # Execution time & memory visualization
│   └── benchmark_report.md            # Human-readable summary report
└── benchmark_execution.log            # Master benchmark log

Configuration Support:
✅ .env file support for default values
✅ Created .env.example with benchmark configuration template
✅ CLI arguments override environment defaults
✅ Support for all pipeline parameters (n_components, anomaly_threshold, max_workers)

Integration:
✅ Wraps existing scripts/run_pipeline.py as specified
✅ Compatible with existing project structure and dependencies
✅ Works with uv tool and Python 3.13.2 environment
✅ Maintains academic-grade documentation standards

Usage Examples:
```bash
# Basic benchmark with 10 runs
uv run scripts/utils/pipeline_benchmark_runner.py data/raw/

# Custom configuration
uv run scripts/utils/pipeline_benchmark_runner.py data/raw/ \
    --runs 5 --output_dir outputs/benchmarks/ \
    --n_components 3 --anomaly_threshold 2.0 --verbose

# With worker limit and temporary file cleanup
uv run scripts/utils/pipeline_benchmark_runner.py data/raw/ \
    --runs 3 --max_workers 4 --keep_tmp --verbose
```

Files Created:
✅ scripts/utils/pipeline_benchmark_runner.py (587 lines) - Main benchmark runner
✅ .env.example (23 lines) - Configuration template

Testing:
✅ CLI interface verified with --help command
✅ All required arguments implemented and documented
✅ Integration with existing pipeline confirmed
✅ Ready for production benchmark execution

==========================================
Status: ✅ COMPLETE - Benchmark Script Created
Last Updated: December 2024 - Benchmark Runner Implementation
==========================================

22. STEP 2: LOW-OVERHEAD SYSTEM MONITORING UTILITIES COMPLETED
===============================================================

Task: Implement Low-Overhead System Monitoring Utilities in scripts/utils/monitoring.py
Status: ✅ COMPLETED

Implementation Details:
- Created comprehensive system monitoring module (scripts/utils/monitoring.py) with 533 lines
- All required functionality implemented: start_monitor() / stop_monitor() with background threading
- CPU monitoring using psutil.Process().cpu_percent(interval) at 1 Hz sampling rate
- Memory tracking: RSS, VMS, and peak memory via tracemalloc and resource.getrusage  
- Apple Silicon optimizations: temperature & frequency via powermetrics and sysctl
- Non-root compatible operation with graceful fallbacks for Apple-specific metrics
- Pandas DataFrame output with timestamped resource samples
- Added psutil>=7.0.0 dependency to requirements.txt

Key Features Implemented:
✅ SystemMonitor class with configurable sampling intervals (default 1 Hz)
✅ Background thread monitoring with thread-safe data collection
✅ CPU metrics: process and system-wide usage, per-core utilization (up to 8 cores)
✅ Memory metrics: RSS, VMS, peak tracking via tracemalloc and getrusage
✅ Apple Silicon detection and optimization
✅ Temperature monitoring via powermetrics --samplers smc (root) and sysctl fallback
✅ CPU frequency monitoring via sysctl hw.cpufrequency_max/hw.cpufrequency
✅ Comprehensive error handling with try/except for non-root operation
✅ Pandas DataFrame output with timestamped samples
✅ Convenience functions: start_monitor(), stop_monitor(), get_system_info()
✅ Monitoring overhead benchmarking capabilities
✅ Command-line interface for testing and benchmarking

Apple Silicon Specific Features:
✅ Automatic detection of arm64 architecture on Darwin
✅ powermetrics integration for detailed thermal/frequency data (when root access available)
✅ sysctl fallback for temperature and frequency monitoring (non-root compatible) 
✅ Per-core CPU usage tracking optimized for M-series processors
✅ Native ARM64 performance optimization

Testing Results:
✅ System information collection verified on Apple Silicon M4 Pro
✅ 10-core CPU detection and per-core monitoring confirmed
✅ Memory tracking working: RSS, VMS, peak memory via both tracemalloc and rusage
✅ Apple Silicon detection: True (arm64 + Darwin)
✅ Continuous monitoring at 1 Hz: 5 samples collected in 5 seconds
✅ Monitoring overhead benchmark: <0.1% overhead at various sampling rates
✅ Thread-safe operation confirmed with proper cleanup
✅ Non-root operation verified (Apple metrics gracefully fallback)

Usage Examples:
```python
# Basic monitoring
from scripts.utils.monitoring import start_monitor, stop_monitor
monitor = start_monitor()
# Your code here...
df = stop_monitor(monitor)
print(df.describe())

# Custom sampling rate
monitor = start_monitor(interval=0.5)  # 2 Hz sampling
df = stop_monitor(monitor)

# Get system info
from scripts.utils.monitoring import get_system_info
info = get_system_info()
```

Command Line Usage:
```bash
# Test monitoring
uv run python scripts/utils/monitoring.py --test --duration 5 --interval 1.0

# System information
uv run python scripts/utils/monitoring.py --system-info

# Benchmark overhead
uv run python scripts/utils/monitoring.py --benchmark --duration 10
```

Output DataFrame Columns:
- timestamp: Sample collection time
- cpu_percent: Process CPU usage
- system_cpu_percent: System-wide CPU usage
- cpu_cores_avg: Average per-core usage
- cpu_core_0-7: Individual core usage
- memory_rss: Resident Set Size
- memory_vms: Virtual Memory Size
- memory_percent: Process memory percentage
- system_memory_percent: System memory usage
- peak_memory_tracemalloc: Peak memory via tracemalloc
- peak_memory_rusage: Peak memory via getrusage
- cpu_die_temperature: Apple-specific temperature (when available)
- cpu_frequency_max_mhz: Maximum CPU frequency (when available)

Academic Integration:
✅ Ready for integration with pipeline benchmarking tools
✅ Suitable for CMMSE 2025 academic publication
✅ Low-overhead design for production research workloads
✅ Comprehensive documentation and examples
✅ Thread-safe for integration with existing pipeline tools

Files Created:
✅ scripts/utils/monitoring.py (533 lines) - Complete monitoring module
✅ Updated requirements.txt with psutil>=7.0.0 dependency

==========================================
Status: ✅ COMPLETE - Low-Overhead System Monitoring Implemented
Last Updated: December 2024 - System Monitoring Utilities
==========================================

23. STEP 3: EXECUTE PIPELINE RUNS WITH DETAILED TIMING HOOKS COMPLETED
=======================================================================

Task: Implement detailed pipeline execution with timing hooks inside benchmark script loop
Status: ✅ COMPLETED

Implementation Details:
- Enhanced pipeline_benchmark_runner.py with comprehensive timing extraction and monitoring
- All 6 required components successfully implemented:
  1. ✅ Spawn monitoring thread using scripts.utils.monitoring (start_monitor/stop_monitor)
  2. ✅ Invoke subprocess.run with capture_output=True, text=True for pipeline execution  
  3. ✅ Parse stdout with regex to extract per-stage timing data into dict format
  4. ✅ Stop monitor and save raw monitor CSV to individual run folders
  5. ✅ Collect file sizes for Stage1-4 parquet files and reports directory
  6. ✅ Store all per-run metrics in memory and dump individual JSON per run

Key Features Implemented:
✅ parse_pipeline_timing() function with regex patterns for stage timing extraction
✅ collect_stage_artifacts_sizes() function for file size collection with error handling
✅ Enhanced subprocess execution with stdout/stderr capture for timing extraction
✅ Integration with monitoring utilities for resource tracking per run
✅ Individual run metrics saved as run_metrics.json in each run folder
✅ Stage timing information included in summary statistics and CSV exports
✅ Resource monitoring CSV saved as resource_monitor.csv per run folder
✅ Comprehensive error handling and graceful fallbacks for missing dependencies

Timing Extraction Details:
- Regex pattern: r'✅ Stage (\d+) completed successfully in ([\d.]+) seconds'
- Extracts: stage1_time, stage2_time, stage3_time, stage4_time, stage5_time
- Also captures: total_pipeline_time from summary output
- Tested with sample pipeline output - correctly extracts all timing data

File Size Collection:
- Stage 1: 01_ingested_data.parquet
- Stage 2: 02_preprocessed_data.parquet
- Stage 3: 03_reference_weeks.parquet
- Stage 4: 04_individual_anomalies.parquet
- Reports: All files in reports/ directory
- Includes size_bytes, size_mb, size_gb for each artifact

Monitoring Integration:
- Uses scripts.utils.monitoring.start_monitor() and stop_monitor()
- Graceful fallback if monitoring utilities not available
- Resource data saved as CSV with timestamped system metrics
- Thread-safe monitoring with proper cleanup

Output Structure Enhanced:
outputs/benchmarks/YYYYMMDD_HHMMSS/
├── run_1/, run_2/, ..., run_N/
│   ├── logs/pipeline_execution.log    # Complete stdout/stderr capture
│   ├── data/*.parquet                 # Stage artifacts with size tracking
│   ├── reports/                       # Analysis outputs
│   ├── run_metrics.json              # ✅ Individual run metrics with timing
│   └── resource_monitor.csv          # ✅ Per-run resource monitoring data
└── summary/
    ├── benchmark_summary.json         # ✅ Includes stage timing statistics
    ├── timing_analysis.csv            # ✅ Enhanced with per-stage columns
    ├── performance_plot.png
    └── benchmark_report.md

Testing Results:
✅ CLI interface functional with comprehensive help and examples
✅ Regex timing extraction tested with sample pipeline output
✅ File parsing correctly extracts all 5 stages + total pipeline time
✅ Integration with existing monitoring utilities confirmed
✅ Error handling for missing dependencies working properly
✅ Ready for production benchmark execution

Usage Examples:
```bash
# Basic benchmark with monitoring and timing extraction
uv run scripts/utils/pipeline_benchmark_runner.py data/raw/ --runs 3 --verbose

# Full benchmark with custom parameters
uv run scripts/utils/pipeline_benchmark_runner.py data/raw/ \
    --runs 5 --n_components 5 --anomaly_threshold 2.5 \
    --max_workers 4 --keep_tmp --verbose
```

Academic Integration:
✅ Compatible with CMMSE 2025 research requirements
✅ Detailed per-stage performance analysis capabilities
✅ Resource utilization tracking for Apple Silicon optimization
✅ Comprehensive benchmark reporting for academic publication
✅ Individual run data preservation for statistical analysis

==========================================
Status: ✅ COMPLETE - Detailed Timing Hooks Implemented
Last Updated: December 2024 - Pipeline Execution Enhancement
==========================================
