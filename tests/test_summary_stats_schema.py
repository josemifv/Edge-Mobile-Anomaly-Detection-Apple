#!/usr/bin/env python3
"""
test_summary_stats_schema.py

CMMSE 2025: Test Suite for Summary Statistics JSON Schema Validation
Tests the structure and validity of summary statistics JSON files generated by the benchmark pipeline.
"""

import pytest
import json
import jsonschema
from pathlib import Path
from typing import Dict, Any


# JSON Schema for summary statistics
SUMMARY_STATS_SCHEMA = {
    "type": "object",
    "required": ["metadata", "aggregate_statistics", "stability_analysis"],
    "properties": {
        "metadata": {
            "type": "object",
            "required": ["generation_timestamp", "description"],
            "properties": {
                "generation_timestamp": {"type": "string"},
                "description": {"type": "string"}
            }
        },
        "aggregate_statistics": {
            "type": "object",
            "patternProperties": {
                "^[a-zA-Z_][a-zA-Z0-9_]*$": {  # Metric names
                    "type": "object",
                    "required": ["count", "mean", "median", "std", "min", "max", "coefficient_of_variation", "ci_95_lower", "ci_95_upper", "margin_of_error"],
                    "properties": {
                        "count": {"type": "integer", "minimum": 0},
                        "mean": {"type": "number"},
                        "median": {"type": "number"},
                        "std": {"type": "number", "minimum": 0},
                        "min": {"type": "number"},
                        "max": {"type": "number"},
                        "coefficient_of_variation": {"type": "number"},
                        "ci_95_lower": {"type": "number"},
                        "ci_95_upper": {"type": "number"},
                        "margin_of_error": {"type": "number", "minimum": 0}
                    }
                }
            }
        },
        "stability_analysis": {
            "type": "object",
            "required": ["total_metrics", "metrics_with_valid_cv", "mean_cv_across_metrics", "median_cv_across_metrics", "categories", "detailed_categories"],
            "properties": {
                "total_metrics": {"type": "integer", "minimum": 0},
                "metrics_with_valid_cv": {"type": "integer", "minimum": 0},
                "mean_cv_across_metrics": {"type": "number"},
                "median_cv_across_metrics": {"type": "number"},
                "categories": {
                    "type": "object",
                    "required": ["excellent", "good", "moderate", "high", "very_high"],
                    "properties": {
                        "excellent": {"type": "integer", "minimum": 0},
                        "good": {"type": "integer", "minimum": 0},
                        "moderate": {"type": "integer", "minimum": 0},
                        "high": {"type": "integer", "minimum": 0},
                        "very_high": {"type": "integer", "minimum": 0}
                    }
                },
                "detailed_categories": {
                    "type": "object",
                    "required": ["excellent", "good", "moderate", "high", "very_high"],
                    "properties": {
                        "excellent": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "required": ["metric", "cv", "mean", "std"],
                                "properties": {
                                    "metric": {"type": "string"},
                                    "cv": {"type": "number"},
                                    "mean": {"type": "number"},
                                    "std": {"type": "number", "minimum": 0}
                                }
                            }
                        },
                        "good": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "required": ["metric", "cv", "mean", "std"],
                                "properties": {
                                    "metric": {"type": "string"},
                                    "cv": {"type": "number"},
                                    "mean": {"type": "number"},
                                    "std": {"type": "number", "minimum": 0}
                                }
                            }
                        },
                        "moderate": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "required": ["metric", "cv", "mean", "std"],
                                "properties": {
                                    "metric": {"type": "string"},
                                    "cv": {"type": "number"},
                                    "mean": {"type": "number"},
                                    "std": {"type": "number", "minimum": 0}
                                }
                            }
                        },
                        "high": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "required": ["metric", "cv", "mean", "std"],
                                "properties": {
                                    "metric": {"type": "string"},
                                    "cv": {"type": "number"},
                                    "mean": {"type": "number"},
                                    "std": {"type": "number", "minimum": 0}
                                }
                            }
                        },
                        "very_high": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "required": ["metric", "cv", "mean", "std"],
                                "properties": {
                                    "metric": {"type": "string"},
                                    "cv": {"type": "number"},
                                    "mean": {"type": "number"},
                                    "std": {"type": "number", "minimum": 0}
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}


class TestSummaryStatsSchema:
    """Test suite for summary statistics JSON schema validation."""

    def test_schema_definition_is_valid(self):
        """Test that the JSON schema definition itself is valid."""
        try:
            jsonschema.Draft7Validator.check_schema(SUMMARY_STATS_SCHEMA)
        except jsonschema.SchemaError as e:
            pytest.fail(f"Schema definition is invalid: {e}")

    def test_sample_valid_summary_stats(self):
        """Test validation with a sample valid summary stats structure."""
        sample_valid_data = {
            "metadata": {
                "generation_timestamp": "2024-12-24T17:59:15.123456",
                "description": "Aggregate statistics for benchmark runs"
            },
            "aggregate_statistics": {
                "stage1_time": {
                    "count": 3,
                    "mean": 78.77,
                    "median": 77.12,
                    "std": 6.21,
                    "min": 73.61,
                    "max": 85.59,
                    "coefficient_of_variation": 0.0788,
                    "ci_95_lower": 63.45,
                    "ci_95_upper": 94.09,
                    "margin_of_error": 15.32
                },
                "total_pipeline_time": {
                    "count": 3,
                    "mean": 248.59,
                    "median": 253.10,
                    "std": 11.12,
                    "min": 235.43,
                    "max": 257.24,
                    "coefficient_of_variation": 0.0447,
                    "ci_95_lower": 217.63,
                    "ci_95_upper": 279.55,
                    "margin_of_error": 30.96
                }
            },
            "stability_analysis": {
                "total_metrics": 3,
                "metrics_with_valid_cv": 3,
                "mean_cv_across_metrics": 0.0618,
                "median_cv_across_metrics": 0.0447,
                "categories": {
                    "excellent": 2,
                    "good": 1,
                    "moderate": 0,
                    "high": 0,
                    "very_high": 0
                },
                "detailed_categories": {
                    "excellent": [
                        {"metric": "stage1_time", "cv": 0.0788, "mean": 78.77, "std": 6.21},
                        {"metric": "stage2_time", "cv": 0.0345, "mean": 91.40, "std": 3.15}
                    ],
                    "good": [
                        {"metric": "total_pipeline_time", "cv": 0.0447, "mean": 248.59, "std": 11.12}
                    ],
                    "moderate": [],
                    "high": [],
                    "very_high": []
                }
            }
        }
        
        try:
            jsonschema.validate(sample_valid_data, SUMMARY_STATS_SCHEMA)
        except jsonschema.ValidationError as e:
            pytest.fail(f"Valid sample data failed validation: {e}")

    def test_missing_required_fields(self):
        """Test that missing required fields cause validation failure."""
        invalid_data = {
            "metadata": {
                "generation_timestamp": "2024-12-24T17:59:15.123456"
                # Missing 'description'
            },
            "aggregate_statistics": {},
            "stability_analysis": {
                "stability_categories": {
                    "excellent": [],
                    "good": [],
                    "moderate": [],
                    "high": [],
                    "very_high": []
                },
                "summary_statistics": {
                    "mean_cv": 0.05,
                    "median_cv": 0.04,
                    "total_metrics": 0
                }
            }
        }
        
        with pytest.raises(jsonschema.ValidationError):
            jsonschema.validate(invalid_data, SUMMARY_STATS_SCHEMA)

    def test_invalid_metric_statistics(self):
        """Test that invalid metric statistics cause validation failure."""
        invalid_data = {
            "metadata": {
                "generation_timestamp": "2024-12-24T17:59:15.123456",
                "description": "Test data"
            },
            "aggregate_statistics": {
                "invalid_metric": {
                    "count": 3,
                    "mean": 100.0,
                    "median": 95.0,
                    "std": -5.0,  # Invalid: negative std
                    "min": 90.0,
                    "max": 110.0,
                    "coefficient_of_variation": 0.05,
                    "ci_95_lower": 85.0,
                    "ci_95_upper": 115.0,
                    "margin_of_error": 15.0
                }
            },
            "stability_analysis": {
                "stability_categories": {
                    "excellent": [],
                    "good": [],
                    "moderate": [],
                    "high": [],
                    "very_high": []
                },
                "summary_statistics": {
                    "mean_cv": 0.05,
                    "median_cv": 0.04,
                    "total_metrics": 1
                }
            }
        }
        
        with pytest.raises(jsonschema.ValidationError):
            jsonschema.validate(invalid_data, SUMMARY_STATS_SCHEMA)

    def test_validate_actual_benchmark_results(self):
        """Test validation against actual benchmark results if available."""
        # Look for recent benchmark results
        benchmark_dirs = list(Path("outputs/benchmarks").glob("*")) if Path("outputs/benchmarks").exists() else []
        
        for benchmark_dir in benchmark_dirs:
            summary_stats_file = benchmark_dir / "summary" / "summary_stats.json"
            if summary_stats_file.exists():
                try:
                    with open(summary_stats_file, 'r') as f:
                        data = json.load(f)
                    
                    # Validate against schema
                    jsonschema.validate(data, SUMMARY_STATS_SCHEMA)
                    print(f"âœ… Validated: {summary_stats_file}")
                    
                except (json.JSONDecodeError, jsonschema.ValidationError) as e:
                    pytest.fail(f"Benchmark results validation failed for {summary_stats_file}: {e}")
                
                # Only validate the first found file to avoid redundant testing
                break
        else:
            pytest.skip("No benchmark results found to validate")


def validate_summary_stats_file(file_path: Path) -> Dict[str, Any]:
    """
    Utility function to validate a summary stats JSON file.
    
    Args:
        file_path: Path to the summary_stats.json file
        
    Returns:
        The validated JSON data
        
    Raises:
        jsonschema.ValidationError: If validation fails
        FileNotFoundError: If file doesn't exist
        json.JSONDecodeError: If JSON is malformed
    """
    if not file_path.exists():
        raise FileNotFoundError(f"Summary stats file not found: {file_path}")
    
    with open(file_path, 'r') as f:
        data = json.load(f)
    
    jsonschema.validate(data, SUMMARY_STATS_SCHEMA)
    return data


if __name__ == "__main__":
    # Run basic validation tests
    print("ğŸ§ª Testing Summary Stats JSON Schema...")
    
    test_instance = TestSummaryStatsSchema()
    
    try:
        test_instance.test_schema_definition_is_valid()
        print("âœ… Schema definition is valid")
        
        test_instance.test_sample_valid_summary_stats()
        print("âœ… Sample valid data passes validation")
        
        test_instance.test_missing_required_fields()
        print("âŒ This should have failed (missing required fields)")
    except AssertionError:
        print("âœ… Missing required fields correctly rejected")
    
    try:
        test_instance.test_invalid_metric_statistics()
        print("âŒ This should have failed (invalid statistics)")
    except AssertionError:
        print("âœ… Invalid metric statistics correctly rejected")
    
    try:
        test_instance.test_validate_actual_benchmark_results()
        print("âœ… Actual benchmark results validated")
    except Exception as e:
        print(f"âš ï¸  Benchmark results validation: {e}")
    
    print("ğŸ‰ Schema validation tests completed!")
